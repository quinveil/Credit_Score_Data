{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statistics\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score,classification_report,confusion_matrix, silhouette_score, davies_bouldin_score\n",
    "from scipy.stats import probplot\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_original = pd.read_csv('Credit Score.csv')\n",
    "df_train = df_train_original.copy()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe(exclude=np.number).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop kolom\n",
    "drop_columns = ['ID','Customer_ID','Name','SSN']\n",
    "df_train.drop(drop_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Drop kolom karena outliers\n",
    "# drop_columns = ['Amount_invested_monthly']\n",
    "# df_train.drop(drop_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Drop kolom karena multicolinearity\n",
    "# drop_columns = ['Annual_Income','Monthly_Inhand_Salary']\n",
    "# df_train.drop(drop_columns,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numericals = df_train.select_dtypes(include='number').columns.tolist()\n",
    "categorical = df_train.select_dtypes(include='object').columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns are {numericals}\")\n",
    "print(f\"Categorical columns are {categorical}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detail kolom \n",
    "def get_column_details(df,column):\n",
    "    print(\"Details of\",column,\"column\")\n",
    "    \n",
    "    #Tipe data\n",
    "    print(\"\\nTipe Data: \",df[column].dtype)\n",
    "    \n",
    "    #Kosong atau tidak ?\n",
    "    count_null = df[column].isnull().sum()\n",
    "    if count_null==0:\n",
    "        print(\"\\nTidak ada value yang kosong\")\n",
    "    elif count_null>0:\n",
    "        print(\"\\nAda \",count_null,\" null values\")\n",
    "        \n",
    "    #Get Number of Unique Values\n",
    "    print(\"\\nUnik: \",df[column].nunique())\n",
    "    \n",
    "    #Get Distribution of Column    \n",
    "    print(\"\\nDistribution of column:\\n\")\n",
    "    print(df[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengisi missing value\n",
    "def fill_missing_with(df, column):      \n",
    "    print(\"\\nSebelum diisi:\",df[column].isnull().sum())\n",
    "    \n",
    "    df[column]=df[column].fillna(df[column].mode().values[0])\n",
    "    \n",
    "    print(\"\\nSesudah diisi:\",df[column].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mengisi missing value dengan groupby\n",
    "def fill_groupby(df, column):\n",
    "    groupby='Occupation'      \n",
    "    print(\"\\nSebelum diisi:\",df[column].isnull().sum())\n",
    "    mode_per_group = df.groupby(groupby)[column].transform(lambda x: x.mean())\n",
    "    df[column] = df[column].fillna(mode_per_group)\n",
    "    print(\"\\nSesudah diisi:\",df[column].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing(df, column):\n",
    "    #fill_groupby(df_train, column_name)\n",
    "    fill_missing_with(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding\n",
    "def ubah_label(df, column):\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "    categorical_columns = [column]\n",
    "    # Initialize the LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    df_train[column] = label_encoder.fit_transform(df_train[column])\n",
    "    print(label_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex(df, column, sampah=None,dummy=''):\n",
    "    if sampah!=None:\n",
    "        df[column] = df[column].replace(sampah,dummy)\n",
    "        print(f\"\\nSampah {sampah} is replaced with Blank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#desimal Scaling\n",
    "def dec_scaling(df, column):\n",
    "    df[column] = df[column].values.reshape(len(df), 1)/100\n",
    "    df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pengolahan categorical\n",
    "def cat_process(df, column, sampah=None, dummy=''):\n",
    "    regex(df, column, sampah,dummy)\n",
    "    ubah_label(df, column)\n",
    "    #dec_scaling(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#penanganan outliers menggunakan quantile\n",
    "def anti_outliers(df, column):\n",
    "    \n",
    "    Q1=df[column].quantile(0.25)\n",
    "    Q3=df[column].quantile(0.75)\n",
    "    IQR=Q3-Q1\n",
    "    print(Q1)\n",
    "    print(Q3)\n",
    "    print(IQR)\n",
    "    lower = Q1-1.5*IQR\n",
    "    upper = Q3+1.5*IQR\n",
    "        \n",
    "    df_train[column] = pd.DataFrame(np.where(df_train[column] >= upper, upper, \n",
    "        (np.where(df_train[column] <= lower, lower, df_train[column]))), columns=[column])\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#penanganan outliers dengan winsorize atau persentase\n",
    "from scipy.stats.mstats import winsorize\n",
    "def anti_outliers_winsorize(df, column):\n",
    "    a=df[column]\n",
    "    df[column]=winsorize(a, limits=[0.1, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pengolahan Numerikal\n",
    "def num_process(df, column, strip=None, datatype=None, sampah=None):\n",
    "    #regex(df, column, sampah)\n",
    "    #ngresiki strip\n",
    "    if df[column].dtype == object and strip is not None:\n",
    "        df[column] = df[column].str.strip(strip)\n",
    "        print(f\"\\nTrailing & leading {strip} are removed\")\n",
    "        \n",
    "    #ganti datatype\n",
    "    if datatype is not None:\n",
    "        df[column] = df[column].astype(datatype)\n",
    "        print(f\"\\nDatatype of {column} is changed to {datatype}\")\n",
    "    #anti_outliers(df_train, column_name)\n",
    "    #anti_outliers_winsorize(df_train, column_name)\n",
    "    #dec_scaling(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot outlier\n",
    "def plot(df, column):\n",
    "    plt.figure(figsize=(20,10))\n",
    "    sns.boxplot(df_train[column], color=\"skyblue\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    z = np.abs(stats.zscore(df_train[column]))\n",
    "    print(z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit Score\n",
    "column_name = 'Credit_Score'\n",
    "\n",
    "get_column_details(df_train, column_name)\n",
    "cat_process(df_train, column_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Occupation\n",
    "column_name = 'Occupation'\n",
    "sampah = '_______'\n",
    "\n",
    "cat_process(df_train, column_name,sampah)\n",
    "fill_missing(df_train, column_name)\n",
    "get_column_details(df_train, column_name)\n",
    "df_train\n",
    "df_train['Occupation'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Type_of_Loan\n",
    "column_name = 'Type_of_Loan'\n",
    "sampah = '_______'\n",
    "\n",
    "cat_process(df_train, column_name,sampah)\n",
    "fill_missing(df_train, column_name)\n",
    "get_column_details(df_train, column_name)\n",
    "df_train\n",
    "df_train['Type_of_Loan'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit_Mix\n",
    "column_name = 'Credit_Mix'\n",
    "sampah = '_'\n",
    "\n",
    "cat_process(df_train, column_name,sampah)\n",
    "fill_missing(df_train, column_name)\n",
    "get_column_details(df_train, column_name)\n",
    "df_train\n",
    "df_train['Credit_Mix'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit_History_Age\n",
    "column_name = 'Credit_History_Age'\n",
    "sampah = 'NA'\n",
    "\n",
    "def Month_Converter(val):\n",
    "    if pd.notnull(val):\n",
    "        years = int(val.split(' ')[0])\n",
    "        month = int(val.split(' ')[3])\n",
    "        return (years*12)+month\n",
    "    else:\n",
    "        return val\n",
    "    \n",
    "df_train['Credit_History_Age'] = df_train['Credit_History_Age'].apply(lambda x: Month_Converter(x)).astype(float)\n",
    "fill_missing(df_train, column_name)\n",
    "print(df_train['Credit_History_Age'])\n",
    "df_train['Credit_History_Age'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Payment_Of_Min_Amount\n",
    "column_name = 'Payment_of_Min_Amount'\n",
    "sampah = 'NM'\n",
    "\n",
    "cat_process(df_train, column_name,sampah)\n",
    "fill_missing(df_train, column_name)\n",
    "get_column_details(df_train, column_name)\n",
    "df_train\n",
    "df_train['Payment_of_Min_Amount'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Payment_Behaviour\n",
    "column_name = 'Payment_Behaviour'\n",
    "sampah = '!@9#%8'\n",
    "dummy = 'Unknown'\n",
    "\n",
    "cat_process(df_train, column_name,sampah,dummy)\n",
    "fill_missing(df_train, column_name)\n",
    "get_column_details(df_train, column_name)\n",
    "df_train\n",
    "df_train['Payment_Behaviour'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Month\n",
    "df_train['Month'] = pd.to_datetime(df_train.Month, format='%B').dt.month\n",
    "df_train['Month'].isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age\n",
    "column_name = 'Age'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=int)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multicolinear\n",
    "#Annual Income\n",
    "column_name = 'Annual_Income'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multicolinear\n",
    "#Monthly_Inhand_Salary\n",
    "column_name = 'Monthly_Inhand_Salary'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num_Bank_Accounts\n",
    "column_name = 'Num_Bank_Accounts'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=int)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num_Credit_Card\n",
    "column_name = 'Num_Credit_Card'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=int)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interest_Rate\n",
    "column_name = 'Interest_Rate'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=int)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num_of_Loan\n",
    "column_name = 'Num_of_Loan'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=int)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delay_from_due_date\n",
    "column_name = 'Delay_from_due_date'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=int)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num_of_Delayed_Payment\n",
    "column_name = 'Num_of_Delayed_Payment'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changed_Credit_Limit\n",
    "column_name = 'Changed_Credit_Limit'\n",
    "sampah = '_'\n",
    "regex(df_train, column_name, sampah)\n",
    "df_train['Changed_Credit_Limit']=pd.to_numeric(df_train['Changed_Credit_Limit'], errors='coerce')\n",
    "num_process(df_train, column_name,  strip='_',datatype='float')\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Num_Credit_Inquiries\n",
    "column_name = 'Num_Credit_Inquiries'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outstanding_Debt\n",
    "column_name = 'Outstanding_Debt'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Credit_Utilization_Ratio\n",
    "column_name = 'Credit_Utilization_Ratio'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total_EMI_per_month\n",
    "column_name = 'Total_EMI_per_month'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note : didrop karena outliers\n",
    "#Amount_invested_monthly\n",
    "column_name = 'Amount_invested_monthly'\n",
    "\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Monthly_Balance\n",
    "column_name = 'Monthly_Balance'\n",
    "\n",
    "num_process(df_train, column_name, strip='_', datatype=float)\n",
    "fill_missing(df_train, column_name)\n",
    "plot(df_train, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reduce_pca(df):\n",
    "#     pca=PCA(n_components=0.95)\n",
    "#     numpy_arr=pca.fit_transform(df)\n",
    "#     df_reduced=pd.DataFrame(data=numpy_arr.flatten())\n",
    "#     return df_reduced\n",
    "# reduce_pca(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feature = df_train.drop(columns='Credit_Score')\n",
    "target = df_train[['Credit_Score']]\n",
    "\n",
    "feature_cs_train, feature_cs_test, target_cs_train, target_cs_test = train_test_split(feature, target, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pengecekan VIF (variance inflation factor)\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as vif \n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "X=add_constant(feature_cs_train)\n",
    "\n",
    "vif_df=pd.DataFrame([vif(X.values, i)\n",
    "                     for i in range(X.shape[1])],\n",
    "                    index=X.columns).reset_index()\n",
    "vif_df.columns=['feature','vif_score']\n",
    "vif_df=vif_df.loc[vif_df.feature!='const']\n",
    "vif_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_excel('creditscore_output.xlsx', engine='xlsxwriter')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Input & Output Data\n",
    "X = df_train.drop('Credit_Score',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Normalize Data\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# scaler2 = StandardScaler()\n",
    "# X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans clustering\n",
    "# def kmeans_clustering(X):\n",
    "inertia=[]\n",
    "K=range(2,4)\n",
    "for num_cluster in K:\n",
    "    kmeans=KMeans(n_clusters=num_cluster, max_iter=50, init='k-means++',n_init=10, random_state=0, algorithm=\"elkan\")\n",
    "    kmeans.fit(X)\n",
    "    kluster = kmeans.predict(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    s_score = silhouette_score(X, kluster)\n",
    "    dbi = davies_bouldin_score(X, kluster)\n",
    "    # label_0=X(kluster==0)\n",
    "    # label_1=X(kluster==1)\n",
    "    # label_2=X(kluster==2)\n",
    "        \n",
    "    print(kluster)\n",
    "    # cols =filtered_label0.columns\n",
    "    # plt.scatter(label_0[cols[0]], label_0[cols[1]], color='red' )\n",
    "    # plt.scatter(label_1[cols[0]], label_1[cols[1]], color='black' )\n",
    "        \n",
    "    print(\"Nilai Silhouette Score n_cluster={0} adalah = {1}\".format(num_cluster, s_score))\n",
    "    print(\"Nilai DBI = \",dbi)\n",
    "plt.plot(K, inertia, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(K)\n",
    "plt.show()\n",
    "# kmeans_clustering(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AHC \n",
    "def ahc_clustering(X):\n",
    "    K=range(2,4)\n",
    "    for num_cluster in K:\n",
    "        ahc=AgglomerativeClustering(n_clusters=num_cluster, linkage='single')\n",
    "        ahc.fit(X)\n",
    "        kluster = ahc.labels_\n",
    "        \n",
    "        s_score = silhouette_score(X, kluster)\n",
    "        dbi = davies_bouldin_score(X, kluster)\n",
    "        print(\"Nilai Silhouette Score n_cluster={0} adalah = {1}\".format(num_cluster, s_score))\n",
    "        print(\"Nilai DBI = \",dbi)\n",
    "ahc_clustering(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "350ec87d84c6c58898f5c2e4bbb3396f5f55fe152cf4ea5d1a9ddc7ca48985be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
